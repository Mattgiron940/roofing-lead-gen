#!/usr/bin/env python3
"""
Master Threaded Scraper Controller
Coordinates all threaded scrapers with ScraperAPI and Supabase
"""

import os
import sys
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess

# Add current directory to path for imports
sys.path.append('.')

def run_scraper(scraper_name):
    """Run individual scraper and return results"""
    try:
        start_time = time.time()
        print(f"ðŸš€ Starting {scraper_name}...")
        
        # Run the scraper as subprocess
        result = subprocess.run([
            sys.executable, 
            f"scrapers/{scraper_name}.py"
        ], capture_output=True, text=True, timeout=300)
        
        end_time = time.time()
        runtime = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… {scraper_name} completed in {runtime:.2f}s")
            return {
                'scraper': scraper_name,
                'status': 'success',
                'runtime': runtime,
                'output': result.stdout
            }
        else:
            print(f"âŒ {scraper_name} failed: {result.stderr}")
            return {
                'scraper': scraper_name,
                'status': 'failed',
                'runtime': runtime,
                'error': result.stderr
            }
            
    except subprocess.TimeoutExpired:
        print(f"â° {scraper_name} timed out after 5 minutes")
        return {
            'scraper': scraper_name,
            'status': 'timeout',
            'runtime': 300,
            'error': 'Timeout after 5 minutes'
        }
    except Exception as e:
        print(f"âŒ Error running {scraper_name}: {e}")
        return {
            'scraper': scraper_name,
            'status': 'error',
            'runtime': 0,
            'error': str(e)
        }

def run_parallel_scrapers():
    """Run all scrapers in parallel"""
    
    # Available scrapers
    scrapers = [
        'threaded_cad_scraper',
        'threaded_redfin_scraper', 
        'threaded_permit_scraper'
    ]
    
    print("ðŸŽ¯ MASTER THREADED SCRAPER STARTING")
    print("=" * 60)
    print(f"ðŸ“Š Running {len(scrapers)} scrapers in parallel")
    print(f"ðŸ• Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    results = []
    start_time = time.time()
    
    # Run scrapers in parallel
    with ThreadPoolExecutor(max_workers=3) as executor:
        # Submit all scrapers
        future_to_scraper = {
            executor.submit(run_scraper, scraper): scraper 
            for scraper in scrapers
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_scraper):
            scraper = future_to_scraper[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"âŒ Exception in {scraper}: {e}")
                results.append({
                    'scraper': scraper,
                    'status': 'exception',
                    'runtime': 0,
                    'error': str(e)
                })
    
    end_time = time.time()
    total_runtime = end_time - start_time
    
    # Print comprehensive report
    print_final_report(results, total_runtime)
    
    return results

def print_final_report(results, total_runtime):
    """Print comprehensive final report"""
    print("\n" + "=" * 60)
    print("ðŸŽ¯ MASTER SCRAPER FINAL REPORT")
    print("=" * 60)
    
    # Overall stats
    successful = len([r for r in results if r['status'] == 'success'])
    failed = len([r for r in results if r['status'] != 'success'])
    
    print(f"â±ï¸  Total Runtime: {total_runtime:.2f} seconds")
    print(f"âœ… Successful: {successful}")
    print(f"âŒ Failed: {failed}")
    print(f"ðŸ“Š Total Scrapers: {len(results)}")
    
    # Individual scraper results
    print("\nðŸ“‹ Individual Scraper Results:")
    for result in results:
        status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
        scraper_name = result['scraper'].replace('threaded_', '').replace('_scraper', '').title()
        runtime = result['runtime']
        
        print(f"   {status_emoji} {scraper_name}: {result['status']} ({runtime:.2f}s)")
        
        if result['status'] != 'success' and 'error' in result:
            print(f"      Error: {result['error'][:100]}...")
    
    # Performance stats
    if results:
        avg_runtime = sum(r['runtime'] for r in results) / len(results)
        fastest = min(results, key=lambda x: x['runtime'])
        slowest = max(results, key=lambda x: x['runtime'])
        
        print(f"\nðŸ“ˆ Performance Stats:")
        print(f"   â€¢ Average Runtime: {avg_runtime:.2f}s")
        print(f"   â€¢ Fastest: {fastest['scraper']} ({fastest['runtime']:.2f}s)")
        print(f"   â€¢ Slowest: {slowest['scraper']} ({slowest['runtime']:.2f}s)")
    
    # Supabase status
    print(f"\nðŸ“Š Data Storage:")
    print(f"   â€¢ All scraped data automatically inserted into Supabase")
    print(f"   â€¢ Tables: cad_leads, redfin_leads, permit_leads")
    print(f"   â€¢ Check your Supabase dashboard for real-time data")
    
    print("\nâœ… MASTER SCRAPER COMPLETED!")
    print("=" * 60)

def check_dependencies():
    """Check if all required dependencies are available"""
    try:
        import requests
        import beautifulsoup4
        import supabase
        import dotenv
        print("âœ… All dependencies available")
        return True
    except ImportError as e:
        print(f"âŒ Missing dependency: {e}")
        print("Run: pip install requests beautifulsoup4 supabase python-dotenv")
        return False

def check_environment():
    """Check if environment variables are set"""
    from config import SUPABASE_URL, SUPABASE_KEY, SCRAPER_API_KEY
    
    if not SUPABASE_URL:
        print("âŒ SUPABASE_URL not set in .env")
        return False
    if not SUPABASE_KEY:
        print("âŒ SUPABASE_KEY not set in .env")
        return False
    if not SCRAPER_API_KEY:
        print("âš ï¸  SCRAPER_API_KEY not set, using default")
    
    print("âœ… Environment configuration OK")
    return True

def main():
    """Main execution function"""
    print("ðŸ” Checking dependencies and environment...")
    
    if not check_dependencies():
        return 1
    
    if not check_environment():
        return 1
    
    # Run all scrapers
    results = run_parallel_scrapers()
    
    # Return success if any scrapers succeeded
    successful_count = len([r for r in results if r['status'] == 'success'])
    return 0 if successful_count > 0 else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)